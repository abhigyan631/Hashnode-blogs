---
title: "Understanding Overfitting and Underfitting: The Enemies of Machine Learning"
seoTitle: "Underfitting and overfitting in machine learning"
datePublished: Sat Apr 08 2023 14:25:40 GMT+0000 (Coordinated Universal Time)
cuid: clg82j1n7000109lg17r721xs
slug: understanding-overfitting-and-underfitting-the-enemies-of-machine-learning
cover: https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/f57lx37DCM4/upload/b4a5b25bb371c12c6be56ab23a81f3d1.jpeg
tags: machine-learning, hashnode, 2articles1week, wemakedevs

---

**Introduction:**

Machine learning has revolutionized many areas of business and science, bringing automation, speed, and optimization to tasks that were previously time-consuming or impossible. But while everyone is excited about the possibilities of machine learning, few people talk about the potential pitfalls. Two significant issues that can arise during the development of a machine-learning model are **overfitting and underfitting**. In this article, **we will explore these problems, explain their causes and effects, discuss some use cases, and provide a guide on how to detect and tackle them**.

**What are Overfitting and Underfitting?**

Both overfitting and underfitting are problems that can occur during the training stage of a machine-learning model.

* **Overfitting:** **Overfitting** refers to the modeling of the training data so tightly that it **fits perfectly without any errors**. It occurs **when a model becomes too complex to represent the underlying pattern in the data**, and it **starts fitting noise** rather **than the actual signal**. Therefore, the model essentially **memorizes the training data**, but **it fails to generalize well to new data**. One sign of overfitting is when the **model's training errors are dramatically lower than its testing errors**.
    
* **Underfitting:** **Underfitting**, on the other hand, refers to the modeling of the **training data in such an oversimplified way that it fails to capture the underlying pattern in the data**. The model is **unable to capture the complexity and variation within the data**. Therefore, the **model fails to generalize well to new data**. One sign of underfitting is **when the model's training and testing errors are very high**, **indicating poor performance in both areas**.
    

**Causes and Effects of Overfitting and Underfitting**

The causes and effects of overfitting and underfitting are as follows:

* **Overfitting**
    
    * **Causes:** Overfitting can occur **when a model is too complex relative to the amount of data being trained**. Other reasons may include using **irrelevant or redundant features, poorly selecting hyperparameters, or not regularizing the model**.
        
    * **Effects:** When overfitting occurs, the **model loses its ability to generalize new data and only learns the flaws and errors of the training data.** The **accuracy of the trained model on new datasets, without including the training data, is severely diminished**.
        
* **Underfitting**
    
    * **Causes:** Underfitting most often occurs **when a model is too simple or insufficiently trained with data**. This typically happens when the **number of features being used is too small relative to the total number of samples in the dataset**.
        
    * Effects: When underfitting occurs, the **model is overly simplistic and fails to capture the signal in the data**. It **performs poorly on the training data** and is unlikely to **perform well on new datasets**.
        

**Tools and Techniques to Tackle Overfitting and Underfitting**

**Overfitting and Underfitting** can be combated with a variety of tools and procedures, such as:

* **Cross-validation:** To reduce the risk of overfitting, assess the **model's performance against a fresh set of data** using techniques like **k-fold cross-validation**.
    
* **Regularization:** To prevent overfitting in complex models, regularisation is a strategy. By **introducing a penalty term to the** [**cost function**](https://www.javatpoint.com/cost-function-in-machine-learning) during training, it **manages the model's complexity**.
    
* **Feature Selection:** The process of **choosing pertinent features to be included in the model** is known as feature selection. It can aid in **simplifying the model and guard against overfitting**.
    
* **Ensemble learning**: **Several models are combined using ensemble learning to increase the accuracy of predictions.** These models are typically **trained on various subsets of the data**, which aids in **preventing overfitting**.
    
* **Optimal hyperparameter tuning**: Tuning the model hyperparameters **for the best-predicted performance** requires the use of **optimization techniques like GridSearchCV**.
    
* **Ensure sufficient data:** Ensure that sufficient data is available for the model to generalize well.
    
* **Early stopping:** Early stopping is a regularization method that **stops training the model** **when validation loss is no longer improving**.
    

**Use cases:**

**Medical Diagnosis**

Consider **developing a machine learning model to determine a patient's likelihood of having diabetes** based on a variety of clinical tests. A dataset is **gathered, divided into training and testing sets, and then your model is run**.

The model **may underfit the data if it is overly straightforward** and contains too few characteristics. In other words, it has **weak forecasting ability** and **fails to adequately account for the complexity of the issue**. In practical medical situations, the model **won't be either useful or trustworthy**.

On the other hand, **if the model is too complex and has a large number of features, it might overfit the data**. That is, it **will perform exceptionally well on the training dataset but fails to generalize well to new**, **unseen data**. In this case, the **model may predict that a patient has diabetes** **when they do not** or vice versa.

Thus, achieving the **optimal balance between the model's complexity and performance is crucial**, as it **could directly impact the accuracy of medical diagnosis and treatment**.

**Image Classification**

Suppose you are **building an image classification model** that can classify different types of animals, such as **dogs, cats, and birds**. You gather a **dataset of thousands of labeled images** and feed it to the model.

If the **model is underfitting**, it **might not be able to distinguish the features that differentiate each animal's class**. The model **may fail even to classify simple image patterns, such as cat ears, dog noses, or bird wings**.

On the other hand, **if your model is overfitting, it may try to memorize the features in your training images instead of learning generalizable features to identify animals' classes**. Overfitting may **lead to incorrect classifications** if the **model encounters new and untrained images in the real world**.

In this case, **optimizing the model's architecture and training strategies**, such as using regularization techniques, can **help avoid overfitting and underfitting issues and achieve better image classification accuracy**.

**Explaining using Bias and Variance**

**Bias and variance** in machine learning are connected to the mistakes a **model makes in its training and test sets of data**. **Bias** is the discrepancy **between the true values of the target variable and the expected (or average) prediction of our model**. In other words, it shows how much our **model tends to simplify the data too much or too little**. **High bias** indicates that our **model frequently oversimplifies**.

The **variability of a model's predictions** **for a certain input** is referred to as **variance**. It shows **how much random noise from the training set affects our model**. **High variance** indicates that our **model is overfitting the training set**, which **suggests that it is learning the noise** **in the training set** as **opposed to the underlying patterns or trends**.

**Under-fitting and Over-fitting**

In machine learning, **underfitting and overfitting** are intimately related to bias and variance.

When the **model is overly straightforward** and **unable to discern the underlying trends in the data, underfitting occurs**. As a result of its oversimplification, the model in this instance has a **high bias and a low variance** (as it is unable to make accurate predictions, even with different training data). As an illustration, **fitting a straight line to a dataset that contains non-linear data is likely to underfit the data because the model is too basic to account for the non-linear relationship between the features and the target**.

**Overfitting is the opposite of underfitting**. It occurs when the **model is too complex and fits the noise in the training data instead of the underlying patterns in the data**. In this case, the model has a **low bias** (as it can **capture the underlying patterns in the data**) but **high variance** (as it is **impacted by random noise**). For example, **fitting a high-order polynomial to a small dataset can lead to overfitting**, as the **model will interpolate the noise in the data** even if it **does not represent the underlying trend**.

**Conclusion**

In conclusion, **overfitting and underfitting** are crucial ideas in machine learning that have a **big impact on how well-trained models function**. When a **model is too complicated, overfitting happens**, and **when a model is too simple, underfitting happens**. **Regularization, feature selection, cross-validation, ensemble learning, optimal hyperparameter tuning, early stopping, and making sure there is enough high-quality data** are just a few of the approaches that **can be used to lessen the effects of overfitting and underfitting**. **Overfitting and underfitting** can be addressed by **comprehending the causes and adopting proactive steps**, **lowering their impact on model training,** and **assisting us in developing more reliable solutions**.

Follow [**me**](https://medium.com/@abhi2652254) and say [**hi**](https://www.linkedin.com/in/obhinaba17/)**.**

If you like my articles please do consider contributing to [**ko-fi**](https://ko-fi.com/abhinababanerjee) to help me upskill and contribute more to the community.

**Github:** [**https://github.com/abhigyan631**](https://github.com/abhigyan631)